PPO состоит из двух файлов, первый из которых задает конфигурацию среды, а второй представляет собой сам алгоритм. Цель агента находится на кухне, начальное положение - в спальне. 
Эпизод завершается, когда: робот достигает цели, проходит максимальное количесвто шагов и упирается в препятсвие (опционально, использовала, чтобы понять, когда робот видит препятсвие, а когда нет).
Помимо отрицательной награды за столкновение с препятсвием и завершение эпизода путем достижения максимального количесвта шагов, за каждый шаг, отдаляющий от цели, я начисляю небольшое наказание. 
Также изначальный ревард = - дистанция до цели. Награждение агент получает за каждый шаг, приближающий его к цели и за завершение эпизода путем достижения цели. 

Агент PPO использует две отдельные нейронные сети: актор (actor) и критик (critic).

Актор: 
- На вход подается состояние состояние агента
- имеет два скрытых слоя по 128 нейронов с функцией активации ReLu
- на выход дает вероятности выбора каждого действия

Критик:
- На вход подается состояние состояние агента
- имеет два скрытых слоя по 128 нейронов с функцией активации ReLu
- на выходе оценка ценности текущего состояния


Моменты, которые плохо работают:

1. Препятсвия: После того, как я добавила к лидару распознование камерой некоторые объекты (кресло или ножка стола) он распознает не очень хорошо, часто пытаясь в них подолгу въехать
2. Само обучение: Спустя, например, 50 эпизодов, как мне кажется, поведение робота почти не меняется, и он не пытается доехать до цели каким-то другим путем, кроме как напрямую (а там мебель и стена). 
Ему для достижения цели нужно выехать из команты, проехать по коридору и завернуть на кухню. 


